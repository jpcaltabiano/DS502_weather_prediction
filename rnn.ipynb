{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we calculate difference? https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "\n",
    "Convert wind dirs to degrees for numerical values? https://study.com/cimages/multimages/16/cb_1_copy.jpeg\n",
    "    - No: convert to direction vector: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "    \n",
    "Major issue to resolve - data is not strictly time series as it contains the data from many sites over time, with overlapping time frames. How to prevent model from considering one site's time series data as an extension of the previous site's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('training_data/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not really work accurately\n",
    "locs = pd.read_csv('locations.csv')\n",
    "bbox = (89.868, 176.001, 2.592, -47.309)\n",
    "# bbox = (locs.lon.min(), locs.lon.max(), locs.lat.min(), locs.lat.max())\n",
    "\n",
    "m = plt.imread('map.png')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "ax.scatter(locs.lon, locs.lat, zorder=1, alpha=0.2, c='b', s=10)\n",
    "ax.set_xlim(bbox[0], bbox[1])\n",
    "ax.set_ylim(bbox[2], bbox[3])\n",
    "ax.invert_yaxis()\n",
    "ax.imshow(m, zorder=0, extent=bbox, aspect='equal', origin='lower')\n",
    "\n",
    "locs.lon.min(), locs.lon.max(), locs.lat.min(), locs.lat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.isna().sum() / data.shape[0]) * 100 # what percent of each column is NA?\n",
    "data[\"RainTomorrow\"].value_counts() # our data is not balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary\n",
    "data[\"RainTomorrow\"].replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "data[\"RainToday\"].replace(('Yes', 'No'), (1, 0), inplace=True)\n",
    "data.drop(columns=['Location', 'Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm', 'RISK_MM'], inplace=True)\n",
    "\n",
    "# convert direction strings to degrees\n",
    "data['WindGustDir'].replace(\n",
    "    ('N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW','SW','WSW','W','WNW','NW','NNW'),\n",
    "    (0, 22.5, 45, 67.5, 90, 112.5, 135, 157.5, 180, 202.5, 225, 247.5, 270, 292.5, 315, 337.5),\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data['WindDir9am'].replace(\n",
    "    ('N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW','SW','WSW','W','WNW','NW','NNW'),\n",
    "    (0, 22.5, 45, 67.5, 90, 112.5, 135, 157.5, 180, 202.5, 225, 247.5, 270, 292.5, 315, 337.5),\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data['WindDir3pm'].replace(\n",
    "    ('N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW','SW','WSW','W','WNW','NW','NNW'),\n",
    "    (0, 22.5, 45, 67.5, 90, 112.5, 135, 157.5, 180, 202.5, 225, 247.5, 270, 292.5, 315, 337.5),\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na in listed columns\n",
    "data.dropna(\n",
    "    axis=0, how='any',\n",
    "    subset=['WindGustDir','WindDir9am','WindDir3pm','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "data_date = pd.to_datetime(data.pop('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features over time\n",
    "plot_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed']\n",
    "# plot_cols = ['Rainfall']\n",
    "plot_features = data[plot_cols]\n",
    "plot_features.index = data_date\n",
    "plot_features.plot(subplots=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of wind data\n",
    "plt.hist2d(data['WindGustDir'], data['WindGustSpeed'], bins=(50, 50), vmax=400)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind Direction [deg]')\n",
    "plt.ylabel('Wind Velocity [m/s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all wind data to vectors\n",
    "wv = data.pop('WindGustSpeed')\n",
    "wd = data.pop('WindGustDir')*np.pi/180\n",
    "data['WindGustX'] = wv*np.cos(wd)\n",
    "data['WindGustY'] = wv*np.sin(wd)\n",
    "\n",
    "wv = data.pop('WindSpeed9am')\n",
    "wd = data.pop('WindDir9am')*np.pi/180\n",
    "data['Wind9amX'] = wv*np.cos(wd)\n",
    "data['Wind9amY'] = wv*np.sin(wd)\n",
    "\n",
    "wv = data.pop('WindSpeed3pm')\n",
    "wd = data.pop('WindDir3pm')*np.pi/180\n",
    "data['Wind3pmX'] = wv*np.cos(wd)\n",
    "data['Wind3pmY'] = wv*np.sin(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of new wind vectors\n",
    "plt.hist2d(data['WindGustX'], data['WindGustY'], bins=(50, 50), vmax=400)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind Direction [deg]')\n",
    "plt.ylabel('Wind Velocity [m/s]')\n",
    "ax = plt.gca()\n",
    "ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = data_date.map(datetime.datetime.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "data['daysin'] = np.sin(timestamp_s * (2 * np.pi/day))\n",
    "data['daycos'] = np.cos(timestamp_s * (2 * np.pi/day))\n",
    "data['yearsin'] = np.sin(timestamp_s * (2 * np.pi/year))\n",
    "data['yearcos'] = np.cos(timestamp_s * (2 * np.pi/year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and normalize all the data\n",
    "n = len(data)\n",
    "train_df = data[0:int(n*0.7)]\n",
    "val_df = data[int(n*0.7):int(n*0.9)]\n",
    "test_df = data[int(n*0.9):]\n",
    "\n",
    "cols_to_norm = list(data.columns)\n",
    "cols_to_norm.remove('RainToday')\n",
    "cols_to_norm.remove('RainTomorrow')\n",
    "\n",
    "train_mean = train_df[cols_to_norm].mean()\n",
    "train_std = train_df[cols_to_norm].std()\n",
    "\n",
    "train_df.loc[:,cols_to_norm] = (train_df.loc[:,cols_to_norm] - train_mean) / train_std\n",
    "val_df.loc[:,cols_to_norm] = (val_df.loc[:,cols_to_norm] - train_mean) / train_std\n",
    "test_df.loc[:,cols_to_norm] = (test_df.loc[:,cols_to_norm] - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = (data - train_mean) / train_std\n",
    "data_melted = data_std.melt(var_name='Column', value_name='Normalized')\n",
    "var_rows = int(data_melted.shape[0]/21) # number of rows for each variable\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=data_melted[:var_rows*6])\n",
    "_ = ax.set_xticklabels(data.keys()[:6], rotation=90)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=data_melted[var_rows*6:var_rows*11])\n",
    "_ = ax.set_xticklabels(data.keys()[6:11], rotation=90)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=data_melted[var_rows*11:var_rows*16])\n",
    "_ = ax.set_xticklabels(data.keys()[11:16], rotation=90)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=data_melted[var_rows*16:var_rows*21])\n",
    "_ = ax.set_xticklabels(data.keys()[16:21], rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None, batch_size=32):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    def plot(self, model=None, plot_col='RainTomorrow', max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        predictions = 0\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(3, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "        \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WindowGenerator(input_width = 7, label_width=1, shift=1, label_columns=['RainTomorrow'])\n",
    "w.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in w.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "linear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "\n",
    "def compile_and_fit(model, window, lr=0.001, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "    model.compile(loss=tf.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(lr=lr),\n",
    "        metrics=[tf.metrics.BinaryAccuracy()])\n",
    "#     model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(),\n",
    "#         metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    history = model.fit(window.train, epochs=epochs, validation_data=window.val, callbacks=[early_stopping])\n",
    "    return history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(linear_model, w)\n",
    "\n",
    "val_performance['Linear'] = linear_model.evaluate(w.val)\n",
    "performance['Linear'] = linear_model.evaluate(w.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "history = compile_and_fit(dense_model, w)\n",
    "val_performance['Dense'] = dense_model.evaluate(w.val)\n",
    "performance['Dense'] = dense_model.evaluate(w.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = w.plot(dense_model)\n",
    "# print(preds)\n",
    "\n",
    "preds = dense_model.predict_classes(w.train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = w.train_df.reset_index()['RainTomorrow']\n",
    "count = 0\n",
    "\n",
    "# for i in range(100,200):\n",
    "#     print(preds[i][0])\n",
    "\n",
    "for i in range(1,preds.shape[0]):\n",
    "# for i in range(100,200):\n",
    "#     print(preds[i][0])\n",
    "#     print(tmp[i])\n",
    "    if ((preds[i][0]-tmp[i])**2) == 0:\n",
    "        count+=1\n",
    "print((preds[1][0] - tmp[1])^2)  \n",
    "count/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WindowGenerator(input_width = 7, label_width=1, shift=1, label_columns=['RainTomorrow'])\n",
    "multi_dense_model = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "    keras.layers.Reshape([1, -1])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_dense_model, w)\n",
    "val_performance['Multi step dense'] = multi_dense_model.evaluate(w.val)\n",
    "performance['Multi step dense'] = multi_dense_model.evaluate(w.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_width = 7\n",
    "w = WindowGenerator(input_width = c_width, label_width=1, shift=1, label_columns=['RainTomorrow'])\n",
    "cnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=c_width, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "history = compile_and_fit(cnn_model, w)\n",
    "val_performance['Conv'] = cnn_model.evaluate(w.val)\n",
    "\n",
    "w.plot(cnn_model)\n",
    "inputs = w.train\n",
    "preds = cnn_model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "--------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "train_df = data[0:int(n*0.7)]\n",
    "val_df = data[int(n*0.7):int(n*0.9)]\n",
    "test_df = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr=0.000001\n",
    "batch_size = 32\n",
    "\n",
    "w = WindowGenerator(input_width = 30, label_width=30, shift=1, label_columns=['RainTomorrow'])\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(16, return_sequences=True),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model, w, lr=lr)\n",
    "val_performance['lstm_model'] = lstm_model.evaluate(w.val)\n",
    "performance['lstm_model'] = lstm_model.evaluate(w.test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "history.history"
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
